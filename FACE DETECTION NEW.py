{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2645b417-98da-4066-83ed-caf5d536a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the family member's name (max 10 members allowed):  Shristi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved image 1 for Shristi\n",
      "✅ Saved image 2 for Shristi\n",
      "✅ Saved image 3 for Shristi\n",
      "✅ Saved image 4 for Shristi\n",
      "✅ Saved image 5 for Shristi\n",
      "✅ Saved image 6 for Shristi\n",
      "✅ Saved image 7 for Shristi\n",
      "✅ Saved image 8 for Shristi\n",
      "✅ Saved image 9 for Shristi\n",
      "✅ Saved image 10 for Shristi\n",
      "✅ All 10 images saved for Shristi in family_dataset\\Shristi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"family_dataset\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)\n",
    "\n",
    "# Ask for the family member's name\n",
    "member_name = input(\"Enter the family member's name (max 10 members allowed): \").strip()\n",
    "member_folder = os.path.join(dataset_path, member_name)\n",
    "\n",
    "if not os.path.exists(member_folder):\n",
    "    os.makedirs(member_folder)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "count = 0  # Track the number of saved images\n",
    "\n",
    "while count < 10:  # Capture 10 images per family member\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Webcam access failed.\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        face_image = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Save image with a timestamp\n",
    "        image_name = f\"{member_name}_{int(time.time())}.jpg\"\n",
    "        cv2.imwrite(os.path.join(member_folder, image_name), face_image)\n",
    "        print(f\"✅ Saved image {count + 1} for {member_name}\")\n",
    "\n",
    "        count += 1  # Increment count\n",
    "\n",
    "    cv2.imshow(\"Capturing Faces\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"✅ All 10 images saved for {member_name} in {member_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719c2ae4-476d-4689-b60e-923a7ebf6826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete! Model saved as face_recognition_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"family_dataset\"\n",
    "\n",
    "# Prepare data\n",
    "X = []  # Images\n",
    "y = []  # Labels (Family Member Names)\n",
    "\n",
    "label_map = {}  # Maps names to numeric labels\n",
    "current_label = 0\n",
    "\n",
    "# Read images for each family member\n",
    "for person in os.listdir(dataset_path):\n",
    "    person_path = os.path.join(dataset_path, person)\n",
    "    \n",
    "    if os.path.isdir(person_path):\n",
    "        label_map[current_label] = person  # Assign label to name\n",
    "        \n",
    "        for img_name in os.listdir(person_path):\n",
    "            img_path = os.path.join(person_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (100, 100))  # Resize to standard size\n",
    "            X.append(img.flatten())  # Convert image to 1D array\n",
    "            y.append(current_label)\n",
    "        \n",
    "        current_label += 1  # Next label\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train KNN model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Save trained model & label map\n",
    "with open(\"face_recognition_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(knn, f)\n",
    "with open(\"label_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_map, f)\n",
    "\n",
    "print(\"✅ Training complete! Model saved as face_recognition_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e57533-70a6-4106-97ae-856720d07fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "🔎 Final Predicted Age: 22\n",
      "✅ No Content Restriction for this user.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "# Load trained face recognition model\n",
    "with open(\"face_recognition_model.pkl\", \"rb\") as f:\n",
    "    knn = pickle.load(f)\n",
    "with open(\"label_map.pkl\", \"rb\") as f:\n",
    "    label_map = pickle.load(f)\n",
    "\n",
    "# Load UTKFace-trained age prediction model\n",
    "age_model = load_model(\"age_model.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Cross-check age with stabilized value\n",
    "stable_age = 0\n",
    "previous_age = 0\n",
    "age_predictions = []  # List to store age predictions for stabilization\n",
    "content_restricted = False  # Variable to track if content is restricted\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_image = gray[y:y + h, x:x + w]\n",
    "        face_resized = cv2.resize(face_image, (100, 100)).flatten().reshape(1, -1)\n",
    "        \n",
    "        # Recognize face\n",
    "        label_pred = knn.predict(face_resized)[0]\n",
    "        name = label_map.get(label_pred, \"Unknown\")\n",
    "        \n",
    "        # Predict age using UTKFace model\n",
    "        face_rgb = cv2.resize(frame[y:y + h, x:x + w], (64, 64)) / 255.0\n",
    "        face_rgb = np.expand_dims(face_rgb, axis=0)\n",
    "        predicted_age = int(age_model.predict(face_rgb)[0][0])\n",
    "\n",
    "        # Stabilize age prediction (using moving average)\n",
    "        age_predictions.append(predicted_age)\n",
    "        if len(age_predictions) > 5:  # Only consider last 5 predictions for stability\n",
    "            age_predictions.pop(0)\n",
    "        \n",
    "        stable_age = int(np.mean(age_predictions))  # Calculate the stabilized age\n",
    "\n",
    "        # Display results\n",
    "        text = f\"{name} - Age: {stable_age}\"\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "        # Check for content restriction (if age < 18)\n",
    "        if name != \"Unknown\" and stable_age < 18:\n",
    "            content_restricted = True\n",
    "        else:\n",
    "            content_restricted = False\n",
    "\n",
    "    # Show webcam feed with the results\n",
    "    cv2.imshow(\"Face Recognition & Age Detection\", frame)\n",
    "\n",
    "    # Exit condition\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # Print the final result when 'q' is pressed\n",
    "        print(f\"\\n🔎 Final Predicted Age: {stable_age}\")\n",
    "        \n",
    "        if content_restricted:\n",
    "            print(\"🔴 Content is Restricted for this user.\")\n",
    "        else:\n",
    "            print(\"✅ No Content Restriction for this user.\")\n",
    "\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac92de8-3017-46e9-ba61-bf6b862b1625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
